Fight Logs from 17 through 25 don't add anything good to analyze. The only good thing about having done these test is that 
help me realize the impact that the amount of observations have according the hyperparameters chosen for the training.
Another take from this is that the actions done by the agent are allways the same two - one for each array of actions -
doesn't matter if you have 1, 5 or 10 agents. The amount of agents only help to add steps to the simulations, if every 
agent is going to do the same over and over through the testing adding more agents doesn't help much. Also, if one
agent is not helpfull (doesn't learn) adding more agents won't change that fact.
The Unity recommended ranges for the hyperparameters don't work properly whe using a high amount of observations and/or 
a complex enviroment.

Fight Logs from 26 through 29 their not that helpfull. I change the reward system to try let the agent discriminate when it
should be using an attack, it wiould also keep receiving a negative or positive reward according the result of the fight.
I use it more to test the hyperparameters needed for a large quantity of observations. The only thing that I got from these 
trials is that the agent can't learn how it should move without some direct impact on the reward system, the agent acts way 
to random to be learning something - maybe this has to do with some of the hyperparameters -, there's nothing usefull to take
from the tenserboard summaries.

Fight Logs 30 is just going back to the simple version of rewarding positive or negativley regarding the outcome of the fight,
but now with custom hyperparameters. The problem with doing this is that: 
1.- Since the player allways charge against the boss, the boss allways wins if it uses the two attacks that it has every time 
they are ready. Even then the agent uses attack in a random pattern or what it seems at random.
2.- Because of 1, the boss never learns how ot move. It results in doing the same movements and in the same order it was doing 
it in the first battles. Example: If it chooses to "Run Away" constantly in the begining alternating occasionally with "Stand
Still", on the subsequenting fight he is going to do the same "Run Away" doing from time to time "Stand Still".
3.- Since the agent is allways winning, he doesn't realize that actually it only needs to pick "Stand Still" and to the first
attack and second attack on that order every time they are available.

Fight Logs 31. For this particular one I change the approach. Instead of having him learning from loosing or winning, I set up
a reward system around doing the attacks on intervals that simulates the "Thinking Time" between attacks. If it chooses an
attack to early or to late it receives a negative reward and if it does the attack in a good time interval it receives a 
positive one. On this example it does learn but to an extent, it gets to a point in which it decides to do less attacks 
minimizing the negative reward and getting more positive rewards.
Maybe mixing this with the winning and lossing reward system will be attacking every time it is on a good interval and not
only doing five to ten attacks to maximize the positive reward over the negative.
I forgot to record this session but at least it's reflected on the log fights.
The tensorboard summaries of this training make more sense that any other one so far.

Fight Logs 32. I tweak the reward system but it has the same problem as 31, there's a time that the boss stops to attack
even if now it losses reward by loosing. I think the problem is the lenght of the episodes. On the begining they start very short
but when the reward stabilizes close to 0 the episodes starts to be quite longer that at the begining, disrupting the stimated
reward and the actual reward that the agent persives. A fix could be adding lectures to the learning making later episodes 
longer, i.e. take more steps to finish.

Fight Logs 33 - 35. These are the first attempt of going back to basics. The only actions that the "Boss" and the "Player" has are
attacking each other, removing a percentage of the health. While the "Player" has only one attack the "Boss" has three but with 
only using one of them can kill the "Player" and receive its reward. On Log 33 and 35, the "Boss" got to a point in which he 
started to get a constant positive reward and doing just the attack that would give him the best result. While on Log 34 he 
never reach that point, the agent saw the positive reward very scarcely at the begining locking him afterwards on a state in 
which he never got to receive it again.

Fight Logs 36 and 37. First attempt of "teaching" the agent how to move towards the "Player". On these cases it was still set up
the discrete model with two branches, one for the movement actions and the other one for the attacks. None of the two gave good
results.

Fight Logs 38 - 41. The first two cases where using a Continuos vector action while the latter two used Discrete vector action but
with one brach, unlike the past project that used two branches. Both models of vector action where tested with movement actions chosen 
by the agent and without the posibility of choose, meaning the agent allways goes to the "Player" by design (NM show on folder name). 
The Logs 38 and 39 - Continuos case - showed that the agent learns how to get closer to the "Player" but never "learns"
which attack to do, even showing a low amount of attacks done and that also can be seen without the movement. While Logs 40 and 41
- Discrete case - showed that the agent gets to learn which attack to do when it doesn't have to "think" on the movemnt action and
when it have to choose a movement it gets a positive reward from time to time but never gets to a constant state. Given the results
from this point onwards I think the best course of actions is to stick to a Discrete model and improve on the reward system.

Fight Logs 42 and 43. They are opposites regarding the results gotten and what can be seen on the demonstrations. While the first one 
got to a point in which the agent got positive rewards loosly sparse, shown that getting a few negative rewards on that context makes
the agent reformulate the movement actions chosen resulting on loosing the positive rewards that was getting before that - meaning
that it gets sparser with time. But the second one shows that, when it get to a more constant positive reward even if he gets a 
negative reward it doesn't affect to much on the policy that was following and contiues to make the same course of actions that allows
him to keep getting positive rewards on stable pass.

Fight Logs 44 is just a DEMO recording for using on Imitation Learning.

Fight Logs 45 - 47. On these iteration of the project I started testing the training with Imitation. The "G" on the folder is to 
distinguish between Imitation (with "G" in the name) and non-Imitation learning, the letter comes from the structure use for Imitation
learning that is G.A.I.L. The three of them showed that the agent receives more positive rewards from the begining of the training,
making it seem as he's following the demonstration. After a period of time the agent starts to do more random actions, this could be
seem as stop following the demonstration and continuing its progress as a normal training and here is where the Log 46 diverge from
the other two. On Log 46 it can be seen the same as before, getting a sparse positive reward and getting a negative one makes the 
agent reevaluate its policy, making it degenerate the "good" behavior and start to lose rewards. While Logs 45 and 47 when it starts
to go away from the imitation, got to a point in which it started to get a constant positive reward and doing the same set of actions
over and over. The difference between 45 and 47 is that, on the first one at the end of the testing it seems that the agent was still
trying new actions and policies while in the second one the set of actions where more constant making it seem as the policy was also
not changing, given the results I can say that 45 needed more time to have the same results of 47.

Fight Logs 48, 49 and 51. By following some of the aspects of the examples project given by unity, I amplified the amount of steps for doing
the summaries of the episodes. That simple change made the training more constant and better, now is more difficult for the agent to be
stuck on a bad policy. It also help me realize about the big movement observation that I added on Google Drive. I also made some changes
to hyperparameters according to what the description in the documentation is, but it seems as most of the parameters don't influence to 
much after going outside the bounds of the recommended values. 

Fight Logs 50. New DEMO to take in acount the changes to the hyperparameters made. Also change the actions taken during the demonstration,
in this case it start by choosing one time the "RunAway" action, follow it by one of the attacks and then continues by choosing all the time
the "MoveForward" action until it reachs a certain distance to start doing the other attack until killing the player.

Fight Logs 52 - 53. Using the newest DEMO, the behavior deteriorated over time. It start well but not much better than without the Imitation,
and at some point the policy starts to get worser and worser. I don't know how to explain why this is happening. I changed the weight that
the imitation has over the training, meaning that it has to try to behave more like the demonstration instead of trying to figure things
out by normal training. Coincidentally, having it mimicking the demonstration (Log 53) gave worse results over time.